\mu=1  with linear search

Step[1]:  x=[ 8.000000 90.000000 ] optim_fx=-984.155200
Step[2]:  x=[ 6.255004 92.932310 ] optim_fx=-996.693602
Step[3]:  x=[ 2.082236 97.829863 ] optim_fx=-1004.905759
Step[4]:  x=[ 0.905083 99.001762 ] optim_fx=-1005.283151
Step[5]:  x=[ 0.971637 98.928978 ] optim_fx=-1005.288312
Step[6]:  x=[ 0.976890 98.923281 ] optim_fx=-1005.288337
Step[7]:  x=[ 0.976918 98.923250 ] optim_fx=-1005.288337

牛顿Armijo回溯法,,共迭代 7 步
结果：
  x=[ 9.769181e-01 9.892325e+01 ] optim_fx=-1005.288337



Step[1]:  x=[ 1.000000 40.000000 ] optim_fx=-421.255053
Step[2]:  x=[ 1.032871 77.681054 ] optim_fx=-798.390805
Step[3]:  x=[ 1.037574 93.676328 ] optim_fx=-957.303576
Step[4]:  x=[ 1.037526 97.948101 ] optim_fx=-998.444109
Step[5]:  x=[ 1.033614 98.532714 ] optim_fx=-1003.411469
Step[6]:  x=[ 1.018676 98.910198 ] optim_fx=-1005.235957
Step[7]:  x=[ 0.975135 98.933287 ] optim_fx=-1005.284716
Step[8]:  x=[ 0.976915 98.923936 ] optim_fx=-1005.288313
Step[9]:  x=[ 0.976918 98.923255 ] optim_fx=-1005.288337
Step[10]:  x=[ 0.976918 98.923250 ] optim_fx=-1005.288337

牛顿Armijo回溯法,,共迭代 10 步
结果：
  x=[ 9.769181e-01 9.892325e+01 ] optim_fx=-1005.288337






Step[1]:  x=[ 15.000000 68.690000 ] optim_fx=-836.270838
Step[2]:  x=[ 15.006566 78.259960 ] optim_fx=-931.363936
Step[3]:  x=[ 13.698221 83.008948 ] optim_fx=-966.383156
Step[4]:  x=[ 8.670766 91.305575 ] optim_fx=-998.910392
Step[5]:  x=[ 0.329966 99.644118 ] optim_fx=-1004.256876
Step[6]:  x=[ 0.548496 99.406399 ] optim_fx=-1004.903321
Step[7]:  x=[ 0.789054 99.141115 ] optim_fx=-1005.210170
Step[8]:  x=[ 0.940799 98.968385 ] optim_fx=-1005.283294
Step[9]:  x=[ 0.975583 98.925399 ] optim_fx=-1005.288302
Step[10]:  x=[ 0.976916 98.923259 ] optim_fx=-1005.288337
Step[11]:  x=[ 0.976918 98.923250 ] optim_fx=-1005.288337

牛顿Armijo回溯法,,共迭代 11 步
结果：
  x=[ 9.769181e-01 9.892325e+01 ] optim_fx=-1005.288337



Step[1]:  x=[ 10.000000 20.000000 ] optim_fx=-303.641157
Step[2]:  x=[ 23.891723 73.017999 ] optim_fx=-958.394392
Step[3]:  x=[ 11.153740 88.595405 ] optim_fx=-996.698346
Step[4]:  x=[ 4.190159 95.582603 ] optim_fx=-1002.999973
Step[5]:  x=[ 0.759243 99.085990 ] optim_fx=-1005.147228
Step[6]:  x=[ 0.928428 99.001969 ] optim_fx=-1005.229181
Step[7]:  x=[ 0.974513 98.934808 ] optim_fx=-1005.283855
Step[8]:  x=[ 0.976912 98.924095 ] optim_fx=-1005.288301
Step[9]:  x=[ 0.976918 98.923257 ] optim_fx=-1005.288337
Step[10]:  x=[ 0.976918 98.923250 ] optim_fx=-1005.288337

牛顿Armijo回溯法,,共迭代 10 步
结果：
  x=[ 9.769181e-01 9.892325e+01 ] optim_fx=-1005.288337



-------------------------------------------------------
\mu=0.1  with linear search

Step[1]:  x=[ 8.000000 90.000000 ] optim_fx=-973.215520
Step[2]:  x=[ 5.602510 93.949063 ] optim_fx=-990.952586
Step[3]:  x=[ 0.823786 99.034816 ] optim_fx=-998.506637
Step[4]:  x=[ 0.450220 99.524523 ] optim_fx=-999.810029
Step[5]:  x=[ 0.054864 99.929515 ] optim_fx=-1000.044150
Step[6]:  x=[ 0.079557 99.913606 ] optim_fx=-1000.061786
Step[7]:  x=[ 0.095673 99.895328 ] optim_fx=-1000.069933
Step[8]:  x=[ 0.099599 99.890503 ] optim_fx=-1000.070550
Step[9]:  x=[ 0.099767 99.890236 ] optim_fx=-1000.070555
Step[10]:  x=[ 0.099767 99.890235 ] optim_fx=-1000.070555

牛顿Armijo回溯法,,共迭代 10 步
结果：
  x=[ 9.976691e-02 9.989023e+01 ] optim_fx=-1000.070555
Step[1]:  x=[ 1.000000 40.000000 ] optim_fx=-410.225505
Step[2]:  x=[ 1.036833 87.020365 ] optim_fx=-880.724648
Step[3]:  x=[ 1.033828 93.803136 ] optim_fx=-948.453536
Step[4]:  x=[ 1.024963 96.399363 ] optim_fx=-974.270159
Step[5]:  x=[ 1.006677 97.707214 ] optim_fx=-987.115097
Step[6]:  x=[ 0.935459 99.060508 ] optim_fx=-999.425596
Step[7]:  x=[ 0.445829 99.549987 ] optim_fx=-999.844424
Step[8]:  x=[ 0.059233 99.935975 ] optim_fx=-1000.037571
Step[9]:  x=[ 0.083299 99.909414 ] optim_fx=-1000.064507
Step[10]:  x=[ 0.097049 99.893688 ] optim_fx=-1000.070233
Step[11]:  x=[ 0.099693 99.890363 ] optim_fx=-1000.070554
Step[12]:  x=[ 0.099767 99.890235 ] optim_fx=-1000.070555
Step[13]:  x=[ 0.099767 99.890235 ] optim_fx=-1000.070555

牛顿Armijo回溯法,,共迭代 13 步
结果：
  x=[ 9.976691e-02 9.989023e+01 ] optim_fx=-1000.070555
Step[1]:  x=[ 15.000000 68.690000 ] optim_fx=-823.337084
Step[2]:  x=[ 14.982501 80.715766 ] optim_fx=-943.330981
Step[3]:  x=[ 11.237281 88.031523 ] optim_fx=-982.593404
Step[4]:  x=[ 1.891992 97.787565 ] optim_fx=-995.810105
Step[5]:  x=[ 0.831135 99.159323 ] optim_fx=-999.549359
Step[6]:  x=[ 0.069659 99.920745 ] optim_fx=-1000.064728
Step[7]:  x=[ 0.090681 99.899337 ] optim_fx=-1000.070113
Step[8]:  x=[ 0.098939 99.891062 ] optim_fx=-1000.070552
Step[9]:  x=[ 0.099760 99.890242 ] optim_fx=-1000.070555
Step[10]:  x=[ 0.099767 99.890235 ] optim_fx=-1000.070555

牛顿Armijo回溯法,,共迭代 10 步
结果：
  x=[ 9.976691e-02 9.989023e+01 ] optim_fx=-1000.070555
Step[1]:  x=[ 10.000000 20.000000 ] optim_fx=-291.364116
Step[2]:  x=[ 18.623335 52.980398 ] optim_fx=-698.881557
Step[3]:  x=[ 20.450464 65.878770 ] optim_fx=-844.279820
Step[4]:  x=[ 20.114236 74.764264 ] optim_fx=-930.030748
Step[5]:  x=[ 14.251273 85.650202 ] optim_fx=-985.722354
Step[6]:  x=[ 6.955413 92.949451 ] optim_fx=-992.996398
Step[7]:  x=[ 3.279725 96.631461 ] optim_fx=-996.662404
Step[8]:  x=[ 0.023230 99.909829 ] optim_fx=-999.622149
Step[9]:  x=[ 0.025458 99.955257 ] optim_fx=-999.981262
Step[10]:  x=[ 0.034938 99.954733 ] optim_fx=-1000.030557
Step[11]:  x=[ 0.057641 99.932371 ] optim_fx=-1000.057919
Step[12]:  x=[ 0.081980 99.908022 ] optim_fx=-1000.068747
Step[13]:  x=[ 0.096596 99.893406 ] optim_fx=-1000.070504
Step[14]:  x=[ 0.099666 99.890336 ] optim_fx=-1000.070555
Step[15]:  x=[ 0.099767 99.890235 ] optim_fx=-1000.070555
Step[16]:  x=[ 0.099767 99.890235 ] optim_fx=-1000.070555

牛顿Armijo回溯法,,共迭代 16 步
结果：
  x=[ 9.976691e-02 9.989023e+01 ] optim_fx=-1000.070555






